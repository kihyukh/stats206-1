{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# American Community Survey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The American Community Survey (ACS) is a large survey of households\n",
    "and individuals in the United States, carried out by the US\n",
    "government on a continuous basis (around 3.5 million people are\n",
    "contacted per year).  It is arguably the most authoritative source\n",
    "of information about the demographic composition of the US\n",
    "population, and is used for many purposes in academic research,\n",
    "government, public policy, and in private industry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the questions in the ACS are about sensitive topics, and\n",
    "therefore are only released in aggregate form.  The \"public use\n",
    "microsample\" (PUMS) is a set of individual ACS responses that only\n",
    "includes information that has been deemed safe for public release at\n",
    "the individual level.  Here we will work with a subset of the\n",
    "ACS/PUMS data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to refer to the documentation to know what the ACS\n",
    "variable names mean.  The documentation is available\n",
    "[here](https://www.census.gov/programs-surveys/acs/technical-documentation/pums/documentation.html).\n",
    "We are using the 2018 \"1-year\" ACS/PUMS.  The file that contains the\n",
    "variable names is called a \"data dictionary\".  You can download it\n",
    "in various formats from the documentation link above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this course, we are providing a simplified version of the\n",
    "ACS/PUMS data from 2018.  It contains a random subset of the\n",
    "cases and a selected subset of variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that many PUMS variables are described as being \"household\" or\n",
    "\"individual\" variables.  These refer to characteristics of\n",
    "households (one or more people living at the same address) or to\n",
    "characteristics of individual people, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the libraries that we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy.stats as sps\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we load the data from the filesystem into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"/scratch/stats206w21_class_root/stats206w21_class/shared_data/datasets\"\n",
    "df = pd.read_csv(os.path.join(base, \"pums_short.csv.gz\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most datasets in the real world have missing values.  There are many\n",
    "reasons that a value may be missing.  For example, in some cases it\n",
    "makes no sense to calculate a number (e.g. income for a young\n",
    "child); or a person may refuse to answer a question in a survey; or\n",
    "a value may have inadvertently not been collected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Pandas, missing values are represented using the symbol `NaN`,\n",
    "which means \"not a number\".  The method `isnull` tells us which\n",
    "values in a Pandas data structure (a dataframe or series) are null.\n",
    "If we want to know the proportion of observations for each variable\n",
    "that are missing, we can use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results shows us that a few variables, e.g. RNTP, have\n",
    "many missing values, while others, e.g. DIVISION, have no missing\n",
    "values.  The RNTP variable contains information about the amount of\n",
    "rent someone pays, so is always missing if a person does not rent\n",
    "the place where they live.  DIVISION indicates where in the US the\n",
    "person lives.  DIVISION can never be missing because the census\n",
    "bureau always has this information about each respondent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to drop all cases (rows) of a variable that are missing,\n",
    "you can use the `dropna` method.  For example, suppose we want the\n",
    "non-missing values for the amount of money that households pay for\n",
    "rent (RNTP).  As noted above, these values are missing for\n",
    "households that do not rent their place of residence.  We can use\n",
    "the following code to obtain the non-missing values and store them\n",
    "in a series called x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[\"RNTP\"].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that an equivalent syntax to what is shown above is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.loc[:, \"RNTP\"].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If is often useful to \"chain\" `dropna` with other methods or\n",
    "attributes of a series.  For example, if we want to know how many\n",
    "people have a non-missing rent value (i.e. are renters), we can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"RNTP\"].dropna().size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The treatment of missing values in a statistical analysis is a\n",
    "complex topic.  Here we are simply demonstrating how to drop the\n",
    "missing values from the dataset.  Sometimes this is the correct\n",
    "thing to do, but often it is not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary statistics of income"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will turn now to the variable \"HINCP\" which is the household\n",
    "income -- the combined income of everyone living in one household.\n",
    "Note that income is not the same thing as wealth.  Many people have\n",
    "far more wealth than income, and their financial standing is largely\n",
    "a function of their wealth, not of their income.\n",
    "\n",
    "Overall, the United States has the following income characteristics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"HINCP\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary statistics above are computed using the non-missing\n",
    "values of HINCP.  Most Pandas dataframe methods, like `describe`,\n",
    "automatically drop missing values, but functions and methods from\n",
    "other Python packages generally do not, so in some settings you will\n",
    "need to drop the missing values explicitly using `dropna`, like\n",
    "this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"HINCP\"].dropna().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the deciles of the household income in the US:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"HINCP\"].quantile([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have discussed that the median (the 0.5 quantile) is a measure of\n",
    "\"location\" or \"central tendency\".  In this sense, the most typical\n",
    "annual household income in the US in 2018 was 63,000 USD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that a very small fraction of the household income\n",
    "values are negative, as shown below.  This is not an error.  Using\n",
    "standard definitions for income, it is possible for a household (or\n",
    "an individual) to have negative income.  The proportion of\n",
    "households with negative income can be obtained as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.HINCP < 0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many quantile-based measures of dispersion are defined by taking the\n",
    "difference between an upper and a lower quantile that are symmetric\n",
    "around 0.5.  The most widely-used quantile-based measure of\n",
    "dispersion is the \"interquartile range\", or IQR, which is the\n",
    "difference between the 0.75 and 0.25 quantiles.  We can calculate it\n",
    "as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = df[\"HINCP\"].quantile([0.25, 0.75])\n",
    "iqr = q[0.75] - q[0.25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The skew of a distribution reflects the spacing between quantiles.\n",
    "The standard quantile-based measure of skew is based on the spacings\n",
    "among the 0.25, 0.5, and 0.75 quantiles.  Let's start by examining\n",
    "these quantiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = df[\"HINCP\"].quantile([0.25, 0.5, 0.75])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the difference between the 0.25 and 0.5 quantiles is\n",
    "less than the difference between the 0.5 and 0.75 quantiles.  This\n",
    "is indicative of _right skew_.  If the difference between the 0.25\n",
    "and 0.5 quantiles were greater than the difference between the 0.5\n",
    "and 0.75 quantiles, this would be indicative of _left skew_.  In\n",
    "practice, right skew is more common than left skew, but either is\n",
    "possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard quantitative measure for skew based on quantiles is\n",
    "calculated next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(q[0.75] + q[0.25] - 2*q[0.5]) / (q[0.75] - q[0.25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this value is positive, it follows that household income is\n",
    "right-skewed.  Income measures are almost always right-skewed.  The\n",
    "statistical reason that income is skewed is that the lower half of\n",
    "the income distribution is distributed from around 0 to 63,000 USD,\n",
    "while the upper half of the income distribution is distributed from\n",
    "around 63,000 USD to the highest income in the population, which is\n",
    "in the millions of dollars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary statistics of log-transformed income"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with quantitative data such as incomes, it is very\n",
    "common to transform the data prior to doing any analysis.  Log\n",
    "transformations are particularly common because they are very easy\n",
    "to interpret, and capture multiplicative relationships well.  If we\n",
    "use the base 2 logarithm, then the key fact to keep in mind when\n",
    "interpreting log-scale data is that if person A has a 1 unit greater\n",
    "value of the log2 income than person B, then person A has twice as\n",
    "much income as person B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a log (base 2) transformed income variable.  Note\n",
    "that we \"clip\" the income to remove people whose income is less than\n",
    "1 USD, since we don't want to take the logarithm of non-positive\n",
    "values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loginc = np.log2(df[\"HINCP\"].clip(1, np.inf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now calculate the IQR of the log-scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = loginc.quantile([0.25, 0.5, 0.75])\n",
    "q[0.75] - q[0.25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IQR is around 1.8, which means that a person at the 75th\n",
    "percentile of the income distribution has around 2^1.8 ~ 3.5 times\n",
    "greater income compared to a person at the 25th percentile of the\n",
    "income distribution.  This complements what we learned from the IQR\n",
    "of the raw data which showed us that a person at the 75th percentile\n",
    "of the income distributions earns around 79,200 USD more income than\n",
    "a person at the 25th percentile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the IQR for log-scale data is dimension-free whereas the\n",
    "IQR for the untransformed income has units of USD.  The reason for\n",
    "this is that the difference of log-transformed values, $\\log(x) -\n",
    "\\log(y)$ is mathematically equivalent to the logarithm of their\n",
    "ratios $\\log(x/y)$.  When taking a ratio, the units cancel.\n",
    "Therefore the IQR of log transformed data is dimension-free."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the quantile skew of the log transformed income data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(q[0.75] + q[0.25] - 2*q[0.5]) / (q[0.75] - q[0.25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This value is slightly negative (-0.09) whereas the skew of the\n",
    "untransformed data was positive at around 0.21.  It is not our goal\n",
    "here to remove the skew, but it is essentially always the case that\n",
    "log-transforming right-skewed data will reduce the skew, as it has\n",
    "done here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratifying to explain heterogeneity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When considering a population such as the United States, which is\n",
    "extremely heterogeneous, it is usually much more informative to\n",
    "analyze the data after stratifying the population according to the\n",
    "values of factors that explain some of the heterogeneity.  The ACS\n",
    "includes a variable called \"FES\" which partitions the population\n",
    "into 8 subgroups based on household structure.  See the data\n",
    "dictionary for the precise definitions of the groups (note that \"LF\"\n",
    "in the documentation stands for \"labor force\").  These groupings\n",
    "were defined many years ago, and are based on heterosexual family\n",
    "structures.  Other variables added more recently capture information\n",
    "about same-sex family structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we can look at the set of standard summary statistics captured\n",
    "by the `describe` method, but now restricting the calculations to\n",
    "the values within each stratum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"FES\")[\"HINCP\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table above, we see that the wealthiest households, based\n",
    "on the median income, are those including a married couple, with\n",
    "both members of the couple working (group 1).  The least wealthy\n",
    "households are those including a single female who is not working\n",
    "(group 8)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often convenient to re-order the rows of the output in order\n",
    "to sort one of the values.  This can be accomplished as below, where\n",
    "we sort the values according to the median income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"FES\")[\"HINCP\"].describe().sort_values(by=\"50%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the `describe` method does not include the IQR, but we\n",
    "can easily add it ourselves, as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df.groupby(\"FES\")[\"HINCP\"].describe()\n",
    "t[\"iqr\"] = t[\"75%\"] - t[\"25%\"]\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the IQR and median are related -- when the IQR is small,\n",
    "the median tends to be small, and when the IQR is large, the median\n",
    "tends to be large.  This is called a _location/dispersion\n",
    "relationship_.  In absolute terms (using USD as the units), there is\n",
    "more dispersion in the higher income subgroups than in the lower\n",
    "income subgroups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple explanatory factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases we want to stratify on two or more factors that may\n",
    "\"explain\" the variation in a value of interest.  Above we considered\n",
    "household structure as one explanatory factor.  Now we will consider\n",
    "the geographic region where the respondent lives as well.  The\n",
    "Census Bureau has several ways of partitioning by geography, the\n",
    "variable `REGION` here uses four levels (see the data dictionary for\n",
    "what they correspond to).  We will conduct a \"two-way\"\n",
    "stratification, and determine the median income for people living in\n",
    "each combination of a FES level and a REGION level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"FES\", \"REGION\"])[\"HINCP\"].aggregate(np.median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of a `groupby`/`aggregate` expression, such as above, is\n",
    "a new dataframe in which the distinct combinations of the grouping\n",
    "variables (here, \"FES\" and \"REGION\") form the dataframe's _index_.\n",
    "To view these results as a table, it is easier to \"unstack\" the\n",
    "results, which moves one level of the row index to the columns.\n",
    "This is also called \"pivoting\", and can be accomplished as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"FES\", \"REGION\"])[\"HINCP\"].aggregate(np.median).unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may be more informative to view these results with a parallel\n",
    "coordinates plot.  To make this plot, we need to move the `FES` and\n",
    "`REGION` variables from the index to regular columns of the\n",
    "dataframe.  This is done using the `reset_index` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = df.groupby([\"FES\", \"REGION\"])[\"HINCP\"].aggregate(np.median)\n",
    "dx = dx.reset_index()\n",
    "sns.lineplot(x=\"FES\", y=\"HINCP\", hue=\"REGION\", palette=sns.color_palette(\"husl\", 4), data=dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows us that for all household types, regions 1 and 4\n",
    "(northeast and west) have greater median incomes than regions 2 and\n",
    "3 (midwest and south).  Region 1 has the highest median income for\n",
    "household type 1 (a married couple, both members of which are\n",
    "working), while household type 4 (a married couple, neither of whom\n",
    "is working) has slightly higher median income in the west (region\n",
    "4).  Regions 2 and 3 are quite similar for all household types\n",
    "except for type 6 (single males not in the labor force), where the\n",
    "income is somewhat higher in the midwest than in the south."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FES variable is nominal, although there is a partial ordering to\n",
    "the categories.  Lower numbers are assigned to married couple\n",
    "families, and lower numbers are assigned to households in the labor\n",
    "force.  But certain pairs of categories cannot be ordered, for\n",
    "example, a married couple in which only the wife works cannot be\n",
    "ordered relative to a married couple in which only the husband\n",
    "works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we consider an analogous parallel coordinates plot using log\n",
    "scale data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"log_HINCP\"] = np.log2(df[\"HINCP\"].clip(1, np.inf))\n",
    "dx = df.groupby([\"FES\", \"REGION\"]).log_HINCP.aggregate(np.median)\n",
    "dx = dx.reset_index()\n",
    "sns.lineplot(x=\"FES\", y=\"log_HINCP\", hue=\"REGION\", palette=sns.color_palette(\"husl\", 4), data=dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot may inspire us to consider a contrast (difference) of\n",
    "the log scale data between two regions, say regions 1 and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"log_HINCP\"] = np.log2(df[\"HINCP\"].clip(1, np.inf))\n",
    "dx = df.groupby([\"FES\", \"REGION\"]).log_HINCP.aggregate(np.median)\n",
    "dx = dx.unstack()\n",
    "dy = dx[1] - dx[3]\n",
    "dy.name = \"Diff_1_3\"\n",
    "dy = dy.reset_index()\n",
    "sns.lineplot(x=\"FES\", y=\"Diff_1_3\", data=dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that the differences plotted above are not constant over\n",
    "the FES categories indicates that the ratio between incomes in the\n",
    "northeast and the south are not the same for all household types.\n",
    "For example, the typical income for family type 4 (a married couple,\n",
    "neither of whom is working) is only slightly greater in the\n",
    "northeast compared to the south (2^0.12 ~ 9%).  But the typical\n",
    "income for family type 6 (a single non-working male) is over 30%\n",
    "greater in the northeast compared to the south.  It is likely that\n",
    "other demographic factors underly these differences.  For example,\n",
    "non-working married couples are often retired, and social security\n",
    "reduces the dispersion in incomes for such people.  On the other\n",
    "hand it is possible that a single non-working male in the northeast\n",
    "is more likely to be a divorced professional, while a single\n",
    "non-working male in the south is more likely to be a younger man who\n",
    "was never married and has low professional skills.  This is only\n",
    "speculation, additional analysis may help clarify these differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This analysis shows that it is possible to gain insight about two\n",
    "explanatory factors by stratifying the data on the _cross product_\n",
    "consisting of all subgroups defined by pairs of levels of the two\n",
    "factors.  These subgroups are sometimes called _cells_.  It should\n",
    "be clear however that this approach will not \"scale\" very well --\n",
    "cross products for three or more factors will generally produce huge\n",
    "numbers of cells.  We may have too little data per cell to produce\n",
    "meaningful estimates of population quantities, and even if this is\n",
    "not an issue, there will be a huge number of combinations to\n",
    "consider.  The difficulty of _controlling_ for, or stratifying on\n",
    "multiple explanatory factors is one of the major challenges that\n",
    "arises in statistical data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantiles and moments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most commonly encountered measures of location are the mean and\n",
    "the median.  For a perfectly symmetric distribution, the mean and\n",
    "the median are identical.  For a right-skewed distribution, the mean\n",
    "is greater than the median, and for a left-skewed distribution, the\n",
    "median is greater than the mean.  We can compare the mean and the\n",
    "median in the ACS data by calculating these two statistics within\n",
    "each household structure group.  Note that by providing two\n",
    "summarizing functions to `aggregate` in a list, we get a result in\n",
    "which both summarizing functions are calculated for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"FES\")[\"HINCP\"].aggregate([np.mean, np.median])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we see that the mean is greater than the median in every case.\n",
    "This happens with right-skewed data, and as mentioned already,\n",
    "income values are almost always right-skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most commonly encountered measures of scale are the standard\n",
    "deviation, median absolute deviation (MAD) and interquartile range\n",
    "(IQR).  These can all be calculated in Python, but unfortunately\n",
    "they are located in different packages.  This leads to the slightly\n",
    "awkward syntax below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = df.groupby(\"FES\")[\"HINCP\"].aggregate([np.std, \"mad\", sps.iqr])\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `aggregate` with multiple summary functions produces a \"wide\"\n",
    "array of results.  For plotting and further analysis, we want our\n",
    "data to have \"long\" form.  We can do this as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = r.reset_index().melt(id_vars=\"FES\", var_name=\"stat_name\", value_name=\"stat_value\")\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make a parallel coordinates plot of the results, which\n",
    "makes it easier to compare the different measures of scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=\"FES\", y=\"stat_value\", hue=\"stat_name\", palette=sns.color_palette(\"husl\", 3), data=r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residuals and Z-scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residuals are obtained by taking each observation, and subtracting\n",
    "from it a reference value.  Usually this reference value is a\n",
    "measure of location calculated on an appropriate group of related\n",
    "observations.  For example, in the United States (as in almost any\n",
    "country), income varies geographically.  Thus, an appropriate\n",
    "reference value for a household's income may be the median income\n",
    "for the state in which that household is located.  Below we\n",
    "calculate the median log-income values by state using the\n",
    "`transform` method with `groupby`, then subtract the state median\n",
    "log-income values from the individual household log-income levels to\n",
    "produce median residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"linc_resid_med\"] = df[\"log_HINCP\"] - df.groupby(\"ST\")[\"log_HINCP\"].transform(np.median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `groupby` with `transform` is related to using `groupby` with\n",
    "`aggregate`, but differs in one important way.  In the case of\n",
    "`aggregate`, the result is smaller in length than the source data,\n",
    "as it contains one value for each grouping level.  For example, if\n",
    "we are aggregating over the 50 US states, then the result of\n",
    "`aggregate` would have 50 rows.  When using `transform`, the summary\n",
    "results are calculated as with aggregate, but then they are \"spread\"\n",
    "back over the rows of the original dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The median of the median residuals is zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"linc_resid_med\"].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean of the median residuals is not zero in general:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"linc_resid_med\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate the mean residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"linc_resid_mean\"] = df[\"log_HINCP\"] - df.groupby(\"ST\")[\"log_HINCP\"].transform(np.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These residuals have mean zero but their median is not (in general)\n",
    "equal to zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"linc_resid_med\"].mean())\n",
    "print(df[\"linc_resid_med\"].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fully standardize the values to \"Z-scores\", we need to divide the\n",
    "residuals by a measure of scale.  Most commonly, the mean residuals\n",
    "are scaled by the standard deviation, and the median residuals are\n",
    "scaled by either the IQR or the MAD.  This gives us moment based\n",
    "Z-scores and quantile-based Z-scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"linc_z_m\"] = df[\"linc_resid_mean\"] / df.groupby(\"ST\")[\"log_HINCP\"].transform(np.std)\n",
    "df[\"linc_z_q\"] = df[\"linc_resid_med\"] / df.groupby(\"ST\")[\"log_HINCP\"].transform(\"mad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the summary statistics of these Z-scores to confirm\n",
    "that they behave as expected.  Specifically, the moment-based\n",
    "residuals will have mean zero and standard deviation one, and the\n",
    "quantile-based residuals will have median zero and MAD one (the MAD\n",
    "is not shown in the table below).  Note that computers often exhibit\n",
    "small numerical errors since the standard representation of a real\n",
    "number on a computer has finite precision.  Therefore, the mean of\n",
    "the moment based residuals below may be expressed as something like\n",
    "\"2e-14\", which means $2\\times 10^{-14}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"linc_z_m\", \"linc_z_q\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the Z-scores based on moments and quantiles look quite\n",
    "similar, as shown in the scatterplot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"linc_z_m\", y=\"linc_z_q\", data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since log-income is approximately unskewed, this result is expected.\n",
    "If we were working with non-logged income, there would have been a\n",
    "greater discrepancy between the quantile-based and moment-based\n",
    "residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The quantile function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quantile of a dataset can be calculated for any probability\n",
    "point p.  Therefore, we can define the quantile function to be the\n",
    "set of all quantiles, indexed by p.  To plot the quantile function,\n",
    "we choose a finite set of probabilities that approximately cover the\n",
    "interval [0, 1], and calculate the quantile for each.  Below we plot\n",
    "the quantile function for household income in the United States."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.linspace(0.01, 0.99, 99)\n",
    "q = df[\"HINCP\"].quantile(p)\n",
    "sns.lineplot(x=p, y=q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can plot the quantile function for log income, but\n",
    "here we exclude the people whose income is less than 10,000 USD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = df.loc[df[\"HINCP\"] >= 10000, :]\n",
    "q = dx[\"log_HINCP\"].quantile(p)\n",
    "sns.lineplot(x=p, y=q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above tells us a great deal about income distribution in\n",
    "the US.  From the 20th to the 80th percentile, the difference is\n",
    "around 2, and since these data are on the log2 scale, this means\n",
    "that there is around a factor of 4 difference between these two\n",
    "points in the distribution. The quantile function drops sharply for\n",
    "p less than 0.2, and rises sharply for p greater than 0.8, and\n",
    "especially for p greater than 0.9.  The latter reflects the fact\n",
    "that incomes in the highest 10-20 percent of the distribution are\n",
    "much greater than incomes in the lower 80 percent of the\n",
    "distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often informative to compare two or more quantile functions.\n",
    "Below we calculate the quantile function of income within each\n",
    "state.  Then we reduce the dataset to contain only three states,\n",
    "California (6), Michigan (26), and Texas (48).  See the ACS/PUMS\n",
    "data documentation for the complete list of numeric state codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dz = dx.groupby(\"ST\")[\"log_HINCP\"].quantile(p)\n",
    "dz = dz.reset_index()\n",
    "dz = dz.loc[dz[\"ST\"].isin([6, 26, 48]), :]\n",
    "dz = dz.rename(columns={\"level_1\": \"p\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we plot the quantile functions for the three states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=\"p\", y=\"log_HINCP\", style=\"ST\", data=dz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can learn a lot by carefully considering this plot.  First, the\n",
    "medians are given by the heights of the graphs at p=0.5.  California\n",
    "has a substantially greater median than Michigan or Texas, and the\n",
    "median for Texas is slightly greater than the median for Michigan.\n",
    "In fact, California has a greater income at every quantile level\n",
    "than Michigan and Texas, although the difference is small at the\n",
    "lowest values of the probability parameter p.  When this happens, we\n",
    "say that the incomes in California are _stochastically greater_ than\n",
    "the incomes in Michigan and Texas.  The convergence of the quantile\n",
    "functions at the lowest values of p indicates that the poorest\n",
    "people in each state have around the same income, but it is notable\n",
    "that even by the 10th percentile, incomes in California are greater\n",
    "than in the other states."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

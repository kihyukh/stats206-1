% Measurement
% Kerby Shedden
% September 30, 2020

Two central questions in designing a data-driven research study:

* Who to measure (cases/units of analysis)?

* What to measure (variables/measurement)?

"Who to measure" falls under the umbrellas of _sampling_
and _design of experiments_.

We focus on the second issue here -- how to define
and obtain the data for each variable in our dataset.

---

Some things are easy to measure, but most are not...
----------------------------------------------------

* Height (more difficult than you might think...)

* Blood pressure (rapid fluctuation, diurnal effects)

* Genetics (expensive, difficult to interpret)

* Presence of trace amounts, e.g. of contaminants (accuracy near limit of detection)

* Ability (e.g. mathematical) -- issues with testing

* IQ (hard to define, questionable relevance)

* Income (multiple jobs, tips, gig work, income sheltering, privacy issues)

---

Some things are easy to measure, but most are not...
----------------------------------------------------

* Political attitudes (multifaceted, reluctance to discuss)

* Voting behavior (cannot directly observe, desirability bias)

* Personality (intangible, multifaceted, indirection, satisficing)

* Mental health (disorder masks it own measurement)

---

Some terms that arise when discussing measurement
-------------------------------------------------

* _Assay_ -- usually a quantitative chemical or biological measurement

* _Semi-quantitative_ -- a measurement whose accuracy is sufficiently limited
that it is not informative to report it numerically

* _Limit of detection_ -- a point below which it is difficult to accurately
quantify the amount of something

* _Construct_ -- a notion that is defined through a collection of traits that
are interpreted as reflecting an underlying characteristic that is too abstract
to measure directly

* _Facet_ -- a construct that is broad may have multiple facets, e.g.
gregariousness and assertiveness are two facets of extroversion

---

Some terms that arise when discussing measurement
-------------------------------------------------

* _Questionnaire_ -- A collection of questions used to measure one or more
quantitative characteristics (may also be called _instruments_, _inventories_,
_checklists_)

* _Structured interview_ -- collecting data by having a trained interviewer
administer a questionnaire to a subject

* _Satisficing_ -- a respondent who puts minimal effort into providing information

* _Social desirability bias_ -- a respondent who tells the interviewer what they
think the interviewer wants to hear

* _Indirection_ -- Efforts by the subject to mislead the interviewer, or by the
interviewer to mislead the subject

* _Invasive_ -- a type of measurement that involves cutting into the body, usually
entailing more risk for the subject

---

Measurement error
-----------------

Usually there is a discrepancy between the value that you measure, and the
true, exact value that you would ideally measure.  This difference is called
_measurement error_.

Broadly speaking, there are two types of measurement error, _systematic_ and
_random_.

* Systematic measurement error impacts many or most observations in the same
direction.  If you ask people how often they steal from their employer, some
people will be honest, but most who do steal will understate their misdeeds;
very few
will exaggerate it (claim that they steal when they do not).  This is
often called _measurement bias_ or just _bias_.

* Random measurement error impacts every observation independently, and
is roughly symmetrically distributed around zero (i.e. can take positive
and negative values with similar frequency).

---

Measurement error and variation
-------------------------------

Error and variation are usually not the same thing.

Suppose we imagine that people have a true mathematical ability.
We administer a test to each of many research subjects on a single
occasion to measure this ability.

The _error_ is the difference between the test score and the person's
true ability.

Variation is the differences in ability among different people.

Since we cannot observe mathematical ability directly, when we analyze
test scores, say by making a histogram, some of the spread in the histogram
is due to measurement error, and some of it is due to true variation in
mathematical ability.

---

Dealing with measurement error
------------------------------

Random measurement errors can be reduced by averaging (positive
and negative errors cancel out).

Systematic measurement errors usually cannot be reduced by
averaging.  Some alternatives:

* Calibration (adjust relative to a "gold standard")

* Focus on change

* Use indirection when collecting data